import streamlit as st
import google.generativeai as genai
import pypdf
import os
import time
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
from streamlit_local_storage import LocalStorage
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
import faiss

# --- Configuration ---
load_dotenv()
PDF_DIR = "data"
FAISS_INDEX_PATH = "vectordb"
GEMINI_EMBEDDING_MODEL = "models/embedding-001"
GEMINI_GENERATIVE_MODEL = "gemini-1.5-flash"
CHUNK_SIZE = 1500
CHUNK_OVERLAP = 150

# --- Language Configuration ---
LANGUAGES = {
    "en": "English",
    "es": "EspaÃ±ol",
    "am": "áŠ áˆ›áˆ­áŠ›", 
    "ar": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"  
}

TEXTS = {
    "en": {
        "page_title": "Bioinformatics Chat",
        "page_icon": "ğŸ§¬",
        "title": "ğŸ§¬ Know more About Bioinformatics",
        "caption": "Ask me any questions about bioinformatics.",
        "config_header": "Configuration",
        "api_key_label": "Enter Google AI API Key",
        "api_key_help": "Get your key from Google AI Studio. Stored locally in your browser.",
        "clear_history_button": "Clear Chat History",
        "history_cleared_success": "Chat history cleared.",
        "db_management_header": "Database Management",
        "recreate_db_button": "Re-create Knowledge Base",
        "recreate_db_help": "Deletes the current FAISS index ({}) and re-processes PDFs from '{}'. This requires a restart.",
        "deleting_db_spinner": "Attempting to delete existing FAISS index and clear cache...",
        "deleted_db_success": "Deleted FAISS index files in: {}",
        "delete_db_error": "Error deleting FAISS index files in {}: {}. Manual deletion might be required.",
        "db_not_found_info": "FAISS index not found, nothing to delete.",
        "cleared_cache_success": "Cleared Streamlit cache. Please refresh the page or restart the app to rebuild the knowledge base.",
        "delete_db_warning": "Could not delete FAISS index. Cache not cleared. Please restart the app manually.",
        "api_key_needed_info": "Please enter your Google AI API Key in the sidebar to begin.",
        "invalid_api_key_error": "Invalid API Key or configuration error. Please check the key in the sidebar. Error: {}",
        "init_spinner": "Initializing knowledge base... This may take a few minutes the first time.",
        "pdf_load_error": "Failed to load or process PDFs. Cannot initialize chatbot.",
        "init_fail_error": "Knowledge base initialization failed. Please check PDF files, API key, and logs.",
        "chat_input_placeholder": "Ask a question about bioinformatics...",
        "thinking_spinner": "Thinking...",
        "no_context_response": "I couldn't find relevant information in the documents to answer your question.",
        "response_error": "Sorry, I encountered an error while generating the response.",
        "language_header": "Language / Idioma / á‰‹áŠ•á‰‹ / Ø§Ù„Ù„ØºØ©",
        "sources": "Sources",
        "settings_button": "âš™ï¸ Settings"
    },
    "es": {
        "page_title": "Chat de BioinformÃ¡tica",
        "page_icon": "ğŸ§¬",
        "title": "ğŸ§¬ Aprende mÃ¡s sobre BioinformÃ¡tica",
        "caption": "Hazme cualquier pregunta sobre bioinformÃ¡tica.",
        "config_header": "ConfiguraciÃ³n",
        "api_key_label": "Introduce la Clave API de Google AI",
        "api_key_help": "ObtÃ©n tu clave de Google AI Studio. Se guarda localmente en tu navegador.",
        "clear_history_button": "Limpiar Historial de Chat",
        "history_cleared_success": "Historial de chat limpiado.",
        "db_management_header": "GestiÃ³n de Base de Datos",
        "recreate_db_button": "Recrear Base de Conocimiento",
        "recreate_db_help": "Elimina el Ã­ndice FAISS actual ({}) y reprocesa los PDFs de '{}'. Requiere reiniciar.",
        "deleting_db_spinner": "Intentando eliminar el Ã­ndice FAISS existente y limpiar cachÃ©...",
        "deleted_db_success": "Archivos de Ã­ndice FAISS eliminados en: {}",
        "delete_db_error": "Error al eliminar archivos de Ã­ndice FAISS en {}: {}. Puede requerir eliminaciÃ³n manual.",
        "db_not_found_info": "Ãndice FAISS no encontrado, nada que eliminar.",
        "cleared_cache_success": "CachÃ© de Streamlit limpiada. Por favor, actualiza la pÃ¡gina o reinicia la app para reconstruir la base de conocimiento.",
        "delete_db_warning": "No se pudo eliminar el Ã­ndice FAISS. CachÃ© no limpiada. Por favor, reinicia la app manualmente.",
        "api_key_needed_info": "Por favor, introduce tu Clave API de Google AI en la barra lateral para comenzar.",
        "invalid_api_key_error": "Clave API invÃ¡lida o error de configuraciÃ³n. Por favor, revisa la clave en la barra lateral. Error: {}",
        "init_spinner": "Inicializando base de conocimiento... Esto puede tardar unos minutos la primera vez.",
        "pdf_load_error": "Fallo al cargar o procesar PDFs. No se puede inicializar el chatbot.",
        "init_fail_error": "Fallo en la inicializaciÃ³n de la base de conocimiento. Por favor, revisa los archivos PDF, la clave API y los logs.",
        "chat_input_placeholder": "Haz una pregunta sobre bioinformÃ¡tica...",
        "thinking_spinner": "Pensando...",
        "no_context_response": "No pude encontrar informaciÃ³n relevante en los documentos para responder tu pregunta.",
        "response_error": "Lo siento, encontrÃ© un error al generar la respuesta.",
        "language_header": "Idioma / Language / á‰‹áŠ•á‰‹ / Ø§Ù„Ù„ØºØ©",
        "sources": "Fuentes",
        "settings_button": "âš™ï¸ ConfiguraciÃ³n"
    },
    "am": {
        "page_title": "á‰£á‹®áŠ¢áŠ•ááˆ­áˆ›á‰²áŠ­áˆµ á‹á‹­á‹­á‰µ",
        "page_icon": "ğŸ§¬",
        "title": "ğŸ§¬ áˆµáˆˆ á‰£á‹®áŠ¢áŠ•ááˆ­áˆ›á‰²áŠ­áˆµ á‹¨á‰ áˆˆáŒ  á‹­á‹ˆá‰",
        "caption": "áˆµáˆˆ á‰£á‹®áŠ¢áŠ•ááˆ­áˆ›á‰²áŠ­áˆµ áˆ›áŠ•áŠ›á‹áŠ•áˆ áŒ¥á‹«á‰„ á‹­áŒ á‹­á‰áŠá¢",
        "config_header": "áˆ›á‹‹á‰€áˆ­",
        "api_key_label": "á‹¨Google AI áŠ¤á’áŠ á‹­ á‰áˆá á‹«áˆµáŒˆá‰¡",
        "api_key_help": "á‰áˆáá‹áŠ• áŠ¨Google AI Studio á‹«áŒáŠ™á¢ á‰ áŠ áˆ³áˆ½á‹ á‹áˆµáŒ¥ á‰ áŠ áŠ«á‰£á‰¢á‹ á‹­á‰€áˆ˜áŒ£áˆá¢",
        "clear_history_button": "á‹¨á‹á‹­á‹­á‰µ á‰³áˆªáŠ­ áŠ áŒ½á‹³",
        "history_cleared_success": "á‹¨á‹á‹­á‹­á‰µ á‰³áˆªáŠ­ áŒ¸á‹µá‰·áˆá¢",
        "db_management_header": "á‹¨á‹áˆ‚á‰¥ áŒá‰³ áŠ áˆµá‰°á‹³á‹°áˆ­",
        "recreate_db_button": "á‹¨áŠ¥á‹á‰€á‰µ áˆ˜áˆ°áˆ¨á‰µáŠ• áŠ¥áŠ•á‹°áŒˆáŠ“ ááŒ áˆ­",
        "recreate_db_help": "á‹¨áŠ áˆáŠ‘áŠ• á‹¨FAISS áˆ˜áˆ¨áŒƒ áŒ á‰‹áˆš ({}) á‹­áˆ°áˆ­á‹›áˆ áŠ¥áŠ“ á’á‹²áŠ¤áá‰½áŠ• áŠ¨ '{}' áŠ¥áŠ•á‹°áŒˆáŠ“ á‹«áˆµáŠ¬á‹³áˆá¢ áŠ¥áŠ•á‹°áŒˆáŠ“ áˆ›áˆµáŒ€áˆ˜áˆ­ á‹«áˆµáˆáˆáŒˆá‹‹áˆá¢",
        "deleting_db_spinner": "á‹«áˆˆá‹áŠ• á‹¨FAISS áˆ˜áˆ¨áŒƒ áŒ á‰‹áˆš áˆˆáˆ˜áˆ°áˆ¨á‹ áŠ¥áŠ“ áˆ˜áˆ¸áŒáŒ« áˆˆáˆ›áŒ½á‹³á‰µ á‰ áˆ˜áˆáŠ¨áˆ­ áˆ‹á‹­...",
        "deleted_db_success": "á‹¨FAISS áˆ˜áˆ¨áŒƒ áŒ á‰‹áˆš á‹á‹­áˆá‰½ á‰°áˆ°áˆ­á‹˜á‹‹áˆ á‰ : {}",
        "delete_db_error": "á‹¨FAISS áˆ˜áˆ¨áŒƒ áŒ á‰‹áˆš á‹á‹­áˆá‰½áŠ• á‰  {} áˆˆáˆ˜áˆ°áˆ¨á‹ áˆµáˆ…á‰°á‰µá¡ {}. á‰ áŠ¥áŒ… áˆ˜áˆ°áˆ¨á‹ áˆŠá‹«áˆµáˆáˆáŒ á‹­á‰½áˆ‹áˆá¢",
        "db_not_found_info": "á‹¨FAISS áˆ˜áˆ¨áŒƒ áŒ á‰‹áˆš áŠ áˆá‰°áŒˆáŠ˜áˆá£ á‹¨áˆšáˆ°áˆ¨á‹ áˆáŠ•áˆ áŠáŒˆáˆ­ á‹¨áˆˆáˆá¢",
        "cleared_cache_success": "á‹¨Streamlit áˆ˜áˆ¸áŒáŒ« áŒ¸á‹µá‰·áˆá¢ áŠ¥á‰£áŠ­á‹ á‹¨áŠ¥á‹á‰€á‰µ áˆ˜áˆ°áˆ¨á‰±áŠ• áŠ¥áŠ•á‹°áŒˆáŠ“ áˆˆáˆ˜áŒˆáŠ•á‰£á‰µ áŒˆáŒ¹áŠ• á‹«á‹µáˆ± á‹ˆá‹­áˆ áˆ˜á‰°áŒá‰ áˆªá‹«á‹áŠ• áŠ¥áŠ•á‹°áŒˆáŠ“ á‹«áˆµáŒ€áˆáˆ©á¢",
        "delete_db_warning": "á‹¨FAISS áˆ˜áˆ¨áŒƒ áŒ á‰‹áˆšáŠ• áˆ˜áˆ°áˆ¨á‹ áŠ áˆá‰°á‰»áˆˆáˆá¢ áˆ˜áˆ¸áŒáŒ« áŠ áˆáŒ¸á‹³áˆá¢ áŠ¥á‰£áŠ­á‹ áˆ˜á‰°áŒá‰ áˆªá‹«á‹áŠ• á‰ áŠ¥áŒ… áŠ¥áŠ•á‹°áŒˆáŠ“ á‹«áˆµáŒ€áˆáˆ©á¢",
        "api_key_needed_info": "áˆˆáˆ˜áŒ€áˆ˜áˆ­ áŠ¥á‰£áŠ­á‹ á‹¨Google AI áŠ¤á’áŠ á‹­ á‰áˆáá‹áŠ• á‰ áŒáŠ• áŠ áˆáˆŒá‹ á‹áˆµáŒ¥ á‹«áˆµáŒˆá‰¡á¢",
        "invalid_api_key_error": "áˆáŠ­ á‹«áˆáˆ†áŠ á‹¨áŠ¤á’áŠ á‹­ á‰áˆá á‹ˆá‹­áˆ á‹¨áˆ›á‹‹á‰€áˆ­ áˆµáˆ…á‰°á‰µá¢ áŠ¥á‰£áŠ­á‹ á‰ áŒáŠ• áŠ áˆáˆŒá‹ á‹áˆµáŒ¥ á‹«áˆˆá‹áŠ• á‰áˆá á‹«áˆ¨áŒ‹áŒáŒ¡á¢ áˆµáˆ…á‰°á‰µá¡ {}",
        "init_spinner": "á‹¨áŠ¥á‹á‰€á‰µ áˆ˜áˆ°áˆ¨á‰µáŠ• á‰ áˆ›áˆµáŒ€áˆ˜áˆ­ áˆ‹á‹­... á‹­áˆ… áˆˆáˆ˜áŒ€áˆ˜áˆªá‹« áŒŠá‹œ áŒ¥á‰‚á‰µ á‹°á‰‚á‰ƒá‹á‰½áŠ• áˆŠá‹ˆáˆµá‹µ á‹­á‰½áˆ‹áˆá¢",
        "pdf_load_error": "á’á‹²áŠ¤áá‰½áŠ• áˆ˜áŒ«áŠ• á‹ˆá‹­áˆ áˆ›áˆµáŠ¬á‹µ áŠ áˆá‰°áˆ³áŠ«áˆá¢ á‰»á‰µá‰¦á‰µáŠ• áˆ›áˆµáŒ€áˆ˜áˆ­ áŠ á‹­á‰»áˆáˆá¢",
        "init_fail_error": "á‹¨áŠ¥á‹á‰€á‰µ áˆ˜áˆ°áˆ¨á‰µ áˆ›áˆµáŒ€áˆ˜áˆ­ áŠ áˆá‰°áˆ³áŠ«áˆá¢ áŠ¥á‰£áŠ­á‹ á‹¨á’á‹²áŠ¤á á‹á‹­áˆá‰½áŠ•á£ á‹¨áŠ¤á’áŠ á‹­ á‰áˆááŠ• áŠ¥áŠ“ áˆá‹áŒá‰¥ áˆ›áˆµá‰³á‹ˆáˆ»á‹á‰½áŠ• á‹«áˆ¨áŒ‹áŒáŒ¡á¢",
        "chat_input_placeholder": "áˆµáˆˆ á‰£á‹®áŠ¢áŠ•ááˆ­áˆ›á‰²áŠ­áˆµ áŒ¥á‹«á‰„ á‹­áŒ á‹­á‰...",
        "thinking_spinner": "á‰ áˆ›áˆ°á‰¥ áˆ‹á‹­...",
        "no_context_response": "áˆˆáŒ¥á‹«á‰„á‹ áˆ˜áˆáˆµ áˆˆáˆ˜áˆµáŒ á‰µ á‰ áˆ°áŠá‹¶á‰¹ á‹áˆµáŒ¥ á‰°á‹›áˆ›áŒ… áˆ˜áˆ¨áŒƒ áˆ›áŒáŠ˜á‰µ áŠ áˆá‰»áˆáŠ©áˆá¢",
        "response_error": "á‹­á‰…áˆ­á‰³á£ áˆáˆ‹áˆ¹áŠ• á‰ áˆ›áˆ˜áŠ•áŒ¨á‰µ áˆ‹á‹­ áˆ³áˆˆ áˆµáˆ…á‰°á‰µ áŠ áŒ‹áŒ¥áˆáŠ›áˆá¢",
        "language_header": "á‰‹áŠ•á‰‹ / Language / Idioma / Ø§Ù„Ù„ØºØ©",
        "sources": "áˆáŠ•áŒ®á‰½",
        "settings_button": "âš™ï¸ áˆ›á‹‹á‰€áˆ­"
    },
    "ar": {
        "page_title": "Ø¯Ø±Ø¯Ø´Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙŠØ© Ø§Ù„Ø­ÙŠÙˆÙŠØ©",
        "page_icon": "ğŸ§¬",
        "title": "ğŸ§¬ Ø§Ø¹Ø±Ù Ø§Ù„Ù…Ø²ÙŠØ¯ Ø¹Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙŠØ© Ø§Ù„Ø­ÙŠÙˆÙŠØ©",
        "caption": "Ø§Ø³Ø£Ù„Ù†ÙŠ Ø£ÙŠ Ø£Ø³Ø¦Ù„Ø© Ø­ÙˆÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙŠØ© Ø§Ù„Ø­ÙŠÙˆÙŠØ©.",
        "config_header": "Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª",
        "api_key_label": "Ø£Ø¯Ø®Ù„ Ù…ÙØªØ§Ø­ Google AI API",
        "api_key_help": "Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­Ùƒ Ù…Ù† Google AI Studio. ÙŠØªÙ… ØªØ®Ø²ÙŠÙ†Ù‡ Ù…Ø­Ù„ÙŠÙ‹Ø§ ÙÙŠ Ù…ØªØµÙØ­Ùƒ.",
        "clear_history_button": "Ù…Ø³Ø­ Ø³Ø¬Ù„ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©",
        "history_cleared_success": "ØªÙ… Ù…Ø³Ø­ Ø³Ø¬Ù„ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©.",
        "db_management_header": "Ø¥Ø¯Ø§Ø±Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
        "recreate_db_button": "Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©",
        "recreate_db_help": "ÙŠØ­Ø°Ù ÙÙ‡Ø±Ø³ FAISS Ø§Ù„Ø­Ø§Ù„ÙŠ ({}) ÙˆÙŠØ¹ÙŠØ¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª PDF Ù…Ù† '{}'. ÙŠØªØ·Ù„Ø¨ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ØºÙŠÙ„.",
        "deleting_db_spinner": "Ø¬Ø§Ø±ÙŠ Ù…Ø­Ø§ÙˆÙ„Ø© Ø­Ø°Ù ÙÙ‡Ø±Ø³ FAISS Ø§Ù„Ø­Ø§Ù„ÙŠ ÙˆÙ…Ø³Ø­ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª...",
        "deleted_db_success": "ØªÙ… Ø­Ø°Ù Ù…Ù„ÙØ§Øª ÙÙ‡Ø±Ø³ FAISS ÙÙŠ: {}",
        "delete_db_error": "Ø®Ø·Ø£ ÙÙŠ Ø­Ø°Ù Ù…Ù„ÙØ§Øª ÙÙ‡Ø±Ø³ FAISS ÙÙŠ {}: {}. Ù‚Ø¯ ÙŠØªØ·Ù„Ø¨ Ø§Ù„Ø­Ø°Ù Ø§Ù„ÙŠØ¯ÙˆÙŠ.",
        "db_not_found_info": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙÙ‡Ø±Ø³ FAISSØŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø´ÙŠØ¡ Ù„Ø­Ø°ÙÙ‡.",
        "cleared_cache_success": "ØªÙ… Ù…Ø³Ø­ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù€ Streamlit. ÙŠØ±Ø¬Ù‰ ØªØ­Ø¯ÙŠØ« Ø§Ù„ØµÙØ­Ø© Ø£Ùˆ Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©.",
        "delete_db_warning": "ØªØ¹Ø°Ø± Ø­Ø°Ù ÙÙ‡Ø±Ø³ FAISS. Ù„Ù… ÙŠØªÙ… Ù…Ø³Ø­ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª. ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙŠØ¯ÙˆÙŠÙ‹Ø§.",
        "api_key_needed_info": "Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù…ÙØªØ§Ø­ Google AI API Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ Ù„Ù„Ø¨Ø¯Ø¡.",
        "invalid_api_key_error": "Ù…ÙØªØ§Ø­ API ØºÙŠØ± ØµØ§Ù„Ø­ Ø£Ùˆ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙƒÙˆÙŠÙ†. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ÙØªØ§Ø­ ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ. Ø§Ù„Ø®Ø·Ø£: {}",
        "init_spinner": "Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©... Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ Ø¨Ø¶Ø¹ Ø¯Ù‚Ø§Ø¦Ù‚ ÙÙŠ Ø§Ù„Ù…Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰.",
        "pdf_load_error": "ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª PDF. Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªÙ‡ÙŠØ¦Ø© Ø±ÙˆØ¨ÙˆØª Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©.",
        "init_fail_error": "ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ù„ÙØ§Øª PDF ÙˆÙ…ÙØªØ§Ø­ API ÙˆØ§Ù„Ø³Ø¬Ù„Ø§Øª.",
        "chat_input_placeholder": "Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø­ÙˆÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙŠØ© Ø§Ù„Ø­ÙŠÙˆÙŠØ©...",
        "thinking_spinner": "Ø¬Ø§Ø±Ù Ø§Ù„ØªÙÙƒÙŠØ±...",
        "no_context_response": "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ.",
        "response_error": "Ø¹Ø°Ø±Ù‹Ø§ØŒ ÙˆØ§Ø¬Ù‡Øª Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø¯.",
        "language_header": "Ø§Ù„Ù„ØºØ© / Language / Idioma / á‰‹áŠ•á‰‹",
        "sources": "Ø§Ù„Ù…ØµØ§Ø¯Ø±",
        "settings_button": "âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª"
    }
}

# --- Helper Functions ---

@st.cache_data
def load_and_process_pdfs():
    """Loads PDFs from global PDF_DIR, extracts text, and splits into chunks."""
    pdf_directory = PDF_DIR
    all_texts = []
    if not os.path.exists(pdf_directory):
        st.error(f"PDF directory '{pdf_directory}' not found. Please create it and add your PDFs.")
        return None, None

    pdf_files = [f for f in os.listdir(pdf_directory) if f.lower().endswith(".pdf")]
    if not pdf_files:
        st.warning(f"No PDF files found in '{pdf_directory}'.")
        return None, None

    total_files = len(pdf_files)

    for i, filename in enumerate(pdf_files):
        filepath = os.path.join(pdf_directory, filename)
        try:
            reader = pypdf.PdfReader(filepath)
            file_text = ""
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    file_text += page_text + "\n"
            if file_text:
                all_texts.append({"filename": filename, "content": file_text})
            else:
                st.warning(f"Could not extract text from '{filename}'. Skipping.")
        except Exception as e:
            st.error(f"Error reading '{filename}': {e}")

    if not all_texts:
        st.error("No text could be extracted from any PDF files.")
        return None, None

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
    )

    all_chunks = []
    metadatas = []
    chunk_id_counter = 0

    for doc in all_texts:
        chunks = text_splitter.split_text(doc["content"])
        for chunk_idx, chunk in enumerate(chunks):
            all_chunks.append(chunk)
            metadatas.append({"source": doc["filename"], "chunk": chunk_idx})
            chunk_id_counter += 1

    return all_chunks, metadatas

@st.cache_resource(ttl=3600)
def setup_faiss_vector_store(chunks, metadatas, api_key):
    """Creates or loads a FAISS vector store with Gemini embeddings."""
    if not chunks or not metadatas:
        st.error("No text chunks available to create vector store.")
        return None

    try:
        embeddings = GoogleGenerativeAIEmbeddings(model=GEMINI_EMBEDDING_MODEL, google_api_key=api_key)

        if os.path.exists(FAISS_INDEX_PATH):
            try:
                vector_store = FAISS.load_local(
                    FAISS_INDEX_PATH,
                    embeddings,
                    allow_dangerous_deserialization=True
                )
                return vector_store
            except Exception as load_e:
                st.warning(f"Failed to load existing FAISS index ({load_e}). Rebuilding...")

        vector_store = FAISS.from_texts(chunks, embedding=embeddings, metadatas=metadatas)
        vector_store.save_local(FAISS_INDEX_PATH)
        
        return vector_store

    except Exception as e:
        st.error(f"Failed to setup FAISS vector store: {e}")
        return None

def get_relevant_context(query, vector_store, api_key, n_results=5):
    """Retrieves relevant context from FAISS vector store."""
    if not vector_store:
        st.error("Vector store not initialized.")
        return [], []
    try:
        results_with_scores = vector_store.similarity_search_with_score(query, k=n_results)
        context_docs = [doc.page_content for doc, score in results_with_scores]
        context_metadatas = [doc.metadata for doc, score in results_with_scores]
        return context_docs, context_metadatas
    except Exception as e:
        st.error(f"Error retrieving context from FAISS: {e}")
        return [], []

def generate_response(query, context_docs, context_metadatas, api_key, language="en"):
    """Generates response using Gemini based on query, context, and language."""
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(GEMINI_GENERATIVE_MODEL)

        context_string = ""
        sources = set()
        for doc, meta in zip(context_docs, context_metadatas):
            source_file = meta.get('source', 'Unknown source')
            context_string += f"Source: {source_file}\nContent:\n{doc}\n\n---\n\n"
            sources.add(source_file)

        language_name = LANGUAGES.get(language, "English") 
        prompt = f"""You are a helpful assistant knowledgeable in bioinformatics, answering questions based on the provided text snippets.
Directly answer the question using *only* the information available and summarize it with a good way to help people understand it better from the text snippets below.
Do not use any prior knowledge But you can use prior knowledge to support your summerization.
If the answer is not found in the snippets, state that the what you are asking for or mentioning is not in my knowledge databse.
**Please provide the answer in {language_name}.**

Context from documents:
{context_string}

Question:
{query}

Answer ({language_name}):
"""

        response = model.generate_content(prompt)
        response_text = response.text
        return response_text

    except Exception as e:
        st.error(f"{TEXTS[language]['response_error']}: {e}")
        return TEXTS[language]['response_error']
    

# --- Streamlit App ---

# Initialize session state for language and settings visibility
if 'language' not in st.session_state:
    st.session_state.language = 'en'
if 'show_settings' not in st.session_state:
    st.session_state.show_settings = False

# Get current language texts
current_texts = TEXTS[st.session_state.language]

# Use language strings for page config, title, caption
st.set_page_config(page_title=current_texts["page_title"], page_icon=current_texts["page_icon"])
st.title(current_texts["title"])
st.caption(current_texts["caption"])

localS = LocalStorage()

# --- Sidebar ---
with st.sidebar:

    if st.button(current_texts["settings_button"]):
        st.session_state.show_settings = not st.session_state.show_settings

    # Conditionally display settings
    if st.session_state.show_settings:
    
        st.header(current_texts["language_header"])
        selected_lang_code = st.selectbox(
            "Select Language",
            options=list(LANGUAGES.keys()),
            format_func=lambda code: LANGUAGES[code],
            key="lang_select",
            index=list(LANGUAGES.keys()).index(st.session_state.language)
        )
        # Update session state if language changed
        if selected_lang_code != st.session_state.language:
            st.session_state.language = selected_lang_code
            st.rerun()

        st.divider()

        # API Key Configuration
        st.header(current_texts["config_header"])
        api_key_from_storage = localS.getItem("gemini_api_key")
        api_key_input = st.text_input(
            current_texts["api_key_label"],
            type="password",
            key="api_key_input_field",
            help=current_texts["api_key_help"],
            value=api_key_from_storage if api_key_from_storage else ""
        )
        if api_key_input:
            localS.setItem("gemini_api_key", api_key_input)
        api_key = api_key_input if api_key_input else (api_key_from_storage if api_key_from_storage else None)

        st.divider()

    # --- Always Visible Sidebar Items ---

    # Retrieve API key if settings are hidden (needed for initialization check later)
    if not st.session_state.show_settings:
         api_key_from_storage = localS.getItem("gemini_api_key")
         api_key = api_key_from_storage if api_key_from_storage else None

    # Clear Chat History Button
    if st.button(current_texts["clear_history_button"]):
        st.session_state.messages = []
        st.success(current_texts["history_cleared_success"])
    
    # Database Management Section
    st.subheader(current_texts["db_management_header"])
    if st.button(current_texts["recreate_db_button"], help=current_texts["recreate_db_help"].format(FAISS_INDEX_PATH, PDF_DIR)):
        with st.spinner(current_texts["deleting_db_spinner"]):
            delete_success = False
            if os.path.exists(FAISS_INDEX_PATH):
                try:
                    index_file = os.path.join(FAISS_INDEX_PATH, "index.faiss")
                    pkl_file = os.path.join(FAISS_INDEX_PATH, "index.pkl")
                    if os.path.exists(index_file):
                        os.remove(index_file)
                    if os.path.exists(pkl_file):
                        os.remove(pkl_file)
                    try:
                        os.rmdir(FAISS_INDEX_PATH)
                    except OSError:
                         pass
                    st.success(current_texts["deleted_db_success"].format(FAISS_INDEX_PATH))
                    delete_success = True
                except Exception as e:
                    st.error(current_texts["delete_db_error"].format(FAISS_INDEX_PATH, e))
            else:
                st.info(current_texts["db_not_found_info"])
                delete_success = True

            if delete_success:
                st.cache_data.clear()
                st.cache_resource.clear()
                st.success(current_texts["cleared_cache_success"])
            else:
                 st.warning(current_texts["delete_db_warning"])


# --- API Key Handling & Initialization ---

if not api_key:
    st.info(current_texts["api_key_needed_info"])
    st.info("Click the âš™ï¸ Settings button in the sidebar to enter your API key.")
    st.stop()

try:
    genai.configure(api_key=api_key)
    list(genai.list_models())
except Exception as e:
    st.error(current_texts["invalid_api_key_error"].format(e))
    st.stop()

# --- Initialization ---
init_start_time = time.time()

initialization_successful = False
faiss_vector_store = None


with st.spinner(current_texts["init_spinner"]):
    chunks, metadatas = load_and_process_pdfs()
    if chunks and metadatas:
        faiss_vector_store = setup_faiss_vector_store(chunks, metadatas, api_key)
        if faiss_vector_store:
            initialization_successful = True
    else:
        st.error(current_texts["pdf_load_error"])


# Use translated error message
if not initialization_successful or not faiss_vector_store:
    st.error(current_texts["init_fail_error"])
    st.stop()

# --- Chat Interface ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Use translated placeholder
if prompt := st.chat_input(current_texts["chat_input_placeholder"]):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        with st.spinner(current_texts["thinking_spinner"]):
            current_lang = st.session_state.language
            context_docs, context_metadatas = get_relevant_context(prompt, faiss_vector_store, api_key)

            if not context_docs:
                # Use translated response
                full_response = current_texts["no_context_response"]
            else:
                # Pass language to generate_response
                full_response = generate_response(prompt, context_docs, context_metadatas, api_key, current_lang)

            message_placeholder.markdown(full_response)
    st.session_state.messages.append({"role": "assistant", "content": full_response})